{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc9ecef5-9745-4829-bf81-296f108fbca5",
   "metadata": {},
   "source": [
    "## Frequent Itemset and Association Rule Mining\n",
    "\n",
    "Consider the text data from MMDS as an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f7480ef0-f7af-4726-abd8-6a3207793b1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/10/09 23:17:13 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from itertools import combinations\n",
    "\n",
    "spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .appName(\"frequent_itemsets\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "data = [(0, ['cat', 'and', 'dog', 'bits']),\n",
    "        (1, ['yahoo', 'news', 'claims', 'a', 'cat', 'mated', 'with', 'dog', 'and', 'produced', 'viable', 'offspring']),\n",
    "        (2, ['cat', 'killer', 'likely', 'is', 'a', 'big', 'dog']),\n",
    "        (3, ['professional', 'free', 'advice', 'on', 'dog', 'training', 'puppy']),\n",
    "        (4, ['cat', 'and', 'kitten', 'training', 'behavior']),\n",
    "        (5, ['dog', '&', 'cat', 'provides', 'training', 'in', 'eugene', 'oregon']),\n",
    "        (6, ['dog', 'and', 'cat', 'is', 'a', 'slang', 'term', 'used', 'by', 'police', 'officers', 'for', 'male-female', 'relationship']),\n",
    "        (7, ['shop', 'for', 'your', 'show', 'dog', 'grooming', 'and', 'pet', 'supplies'])\n",
    "]\n",
    "\n",
    "df = spark.createDataFrame(data, [\"id\", \"basket\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b305e304-26db-48c2-a428-a1117142ef54",
   "metadata": {},
   "source": [
    "## A Priori Algorithm\n",
    "\n",
    "How can we identify frequent itemsets and associations in this dataset? While the A Priori Algorithm discussed in MMDS is not implemented in Spark, we can write our version. Let's walk through the different steps of the algorithm (as well as how they can be implemented in Spark) and then we can consolidate everything into a single function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dbb3af21-709f-4780-a024-494df65dedd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{('dog',): 7, ('a',): 3, ('training',): 3, ('cat',): 6, ('and',): 5}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = 3\n",
    "# Find frequent singletons\n",
    "L1 = df.rdd.flatMap(lambda x: [((i,), 1) for i in x.basket]) \\\n",
    "           .reduceByKey(lambda x, y: x + y) \\\n",
    "           .filter(lambda x: x[1] >= s) \\\n",
    "           .collect()\n",
    "Lk_1 = dict(L1)\n",
    "freq_itemsets = Lk_1\n",
    "freq_itemsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bdf05b0e-a191-4f14-bc9a-bab59ca5b806",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('and', 'cat'), 1),\n",
       " (('cat', 'dog'), 1),\n",
       " (('and', 'dog'), 1),\n",
       " (('a', 'cat'), 1),\n",
       " (('a', 'dog'), 1),\n",
       " (('a', 'and'), 1),\n",
       " (('cat', 'dog'), 1),\n",
       " (('and', 'cat'), 1),\n",
       " (('and', 'dog'), 1),\n",
       " (('a', 'cat'), 1),\n",
       " (('cat', 'dog'), 1),\n",
       " (('a', 'dog'), 1),\n",
       " (('dog', 'training'), 1),\n",
       " (('and', 'cat'), 1),\n",
       " (('cat', 'training'), 1),\n",
       " (('and', 'training'), 1),\n",
       " (('cat', 'dog'), 1),\n",
       " (('dog', 'training'), 1),\n",
       " (('cat', 'training'), 1),\n",
       " (('and', 'dog'), 1),\n",
       " (('cat', 'dog'), 1),\n",
       " (('a', 'dog'), 1),\n",
       " (('and', 'cat'), 1),\n",
       " (('a', 'and'), 1),\n",
       " (('a', 'cat'), 1),\n",
       " (('and', 'dog'), 1)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C2 = df.rdd.flatMap(lambda x: [i for i in combinations(x.basket, 2)]) \\\n",
    "                   .map(lambda x: (x, set([tuple(sorted(i)) for i in combinations(x, 1)]))) \\\n",
    "                   .filter(lambda x: set(x[1]).issubset(Lk_1)) \\\n",
    "                   .map(lambda x: (tuple(sorted(x[0])), 1))\n",
    "C2.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bd43daf9-669b-49c1-8081-4fab69c6dd96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('a', 'dog'): 3,\n",
       " ('and', 'cat'): 4,\n",
       " ('and', 'dog'): 4,\n",
       " ('cat', 'dog'): 5,\n",
       " ('a', 'cat'): 3}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L2 = C2.reduceByKey(lambda x, y: x + y) \\\n",
    "       .filter(lambda x: x[1] >= s) \\\n",
    "       .collect()\n",
    "\n",
    "L2 = dict(L2)\n",
    "L2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "45f286ab-7156-42c2-aa38-c50181dabcc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('a', 'cat', 'dog'): 3, ('and', 'cat', 'dog'): 3}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C3 = df.rdd.flatMap(lambda x: [i for i in combinations(x.basket, 3)]) \\\n",
    "           .map(lambda x: (x, set([tuple(sorted(i)) for i in combinations(x, 2)]))) \\\n",
    "           .filter(lambda x: set(x[1]).issubset(L2)) \\\n",
    "           .map(lambda x: (tuple(sorted(x[0])), 1)) \\\n",
    "\n",
    "L3 = C3.reduceByKey(lambda x, y: x + y) \\\n",
    "       .filter(lambda x: x[1] >= s) \\\n",
    "       .collect()\n",
    "L3 = dict(L3)\n",
    "L3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5dc43c5-5e3a-4c10-b87a-5d394a3390be",
   "metadata": {},
   "source": [
    "Bringing it all together into a single function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8227c402-cded-46de-9e15-e3cd4abe9f12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('dog',): 7,\n",
       " ('a',): 3,\n",
       " ('training',): 3,\n",
       " ('cat',): 6,\n",
       " ('and',): 5,\n",
       " ('a', 'dog'): 3,\n",
       " ('and', 'cat'): 4,\n",
       " ('and', 'dog'): 4,\n",
       " ('cat', 'dog'): 5,\n",
       " ('a', 'cat'): 3,\n",
       " ('a', 'cat', 'dog'): 3,\n",
       " ('and', 'cat', 'dog'): 3}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Discuss: Critique this implementation\n",
    "# in what scenarios might this fail to scale? What might you do to fix it?\n",
    "def apriori(df, support=3):\n",
    "    # Find frequent singletons\n",
    "    L1 = df.rdd.flatMap(lambda x: [((i,), 1) for i in x.basket]) \\\n",
    "               .reduceByKey(lambda x, y: x + y) \\\n",
    "               .filter(lambda x: x[1] >= s) \\\n",
    "               .collect()\n",
    "    Lk_1 = dict(L1)\n",
    "    freq_itemsets = Lk_1\n",
    "\n",
    "    # Find frequent doubletons and higher\n",
    "    k = 2\n",
    "    while Lk_1:\n",
    "        Lk = df.rdd.flatMap(lambda x: [i for i in combinations(x.basket, k)]) \\\n",
    "                   .map(lambda x: (x, set([tuple(sorted(i)) for i in combinations(x, k - 1)]))) \\\n",
    "                   .filter(lambda x: set(x[1]).issubset(Lk_1)) \\\n",
    "                   .map(lambda x: (tuple(sorted(x[0])), 1)) \\\n",
    "                   .reduceByKey(lambda x, y: x + y) \\\n",
    "                   .filter(lambda x: x[1] >= s) \\\n",
    "                   .collect()\n",
    "        Lk_1 = dict(Lk)\n",
    "        freq_itemsets.update(Lk_1)\n",
    "        k += 1\n",
    "\n",
    "    return freq_itemsets\n",
    "\n",
    "freq_itemsets = apriori(df)\n",
    "freq_itemsets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb353c9d-3ba0-4a2b-a639-5cf1681524a8",
   "metadata": {},
   "source": [
    "...and another function to generate association rules:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b3c751ea-2384-44c6-9e36-63039d8ac9c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('a',), ('dog',), 1.0, 0.125),\n",
       " (('and',), ('cat',), 0.8, 0.050000000000000044),\n",
       " (('and',), ('dog',), 0.8, -0.07499999999999996),\n",
       " (('cat',), ('dog',), 0.8333333333333334, -0.04166666666666663),\n",
       " (('a',), ('cat',), 1.0, 0.25),\n",
       " (('a',), ('dog', 'cat'), 1.0, 1.0),\n",
       " (('a', 'cat'), ('dog',), 1.0, 0.125),\n",
       " (('a', 'dog'), ('cat',), 1.0, 0.25)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def generate_association_rules(freq_itemsets, total_count, confidence_threshold=0.8):\n",
    "    # Serial computation of association rules + confidence/interest\n",
    "    # based on 'freq_itemsets' dictionary and MMDS definitions\n",
    "    rules = []\n",
    "    for itemset, support in freq_itemsets.items():\n",
    "        # Generate all non-empty subsets of the itemset\n",
    "        subsets = [set(x) for i in range(1, len(itemset)) for x in combinations(itemset, i)]\n",
    "        for subset in subsets:\n",
    "            subset = tuple(subset)\n",
    "            remaining = tuple(set(itemset) - set(subset))\n",
    "            if remaining:\n",
    "                # Calculate confidence\n",
    "                subset_support = freq_itemsets.get(subset, 0)\n",
    "                if subset_support > 0:\n",
    "                    confidence = support / subset_support\n",
    "                    if confidence >= confidence_threshold:\n",
    "                        # Calculate interest\n",
    "                        remaining_support = freq_itemsets.get(remaining, 0)\n",
    "                        prob_remaining = remaining_support / total_count\n",
    "                        interest = confidence - prob_remaining\n",
    "                        rules.append((subset, remaining, confidence, interest))\n",
    "    return rules\n",
    "\n",
    "generate_association_rules(freq_itemsets, df.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d6b032c-d70f-4af4-96c2-525d19364c3b",
   "metadata": {},
   "source": [
    "## FP Growth\n",
    "\n",
    "We don't need to exclusively write our own functions for frequent itemset mining, however. PySpark has a built-in parallel implementation of FP Growth -- a highly scalable approach to mining frequent itemsets and association rules:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0b1ad472-1e32-43dd-be72-e7ab7b3b4f43",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+----+\n",
      "|          items|freq|\n",
      "+---------------+----+\n",
      "|          [dog]|   7|\n",
      "|          [cat]|   6|\n",
      "|          [and]|   5|\n",
      "|     [cat, dog]|   5|\n",
      "|     [and, cat]|   4|\n",
      "|     [and, dog]|   4|\n",
      "|     [training]|   3|\n",
      "|[and, cat, dog]|   3|\n",
      "|            [a]|   3|\n",
      "|  [a, cat, dog]|   3|\n",
      "|       [a, cat]|   3|\n",
      "|       [a, dog]|   3|\n",
      "+---------------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.fpm import FPGrowth\n",
    "\n",
    "fp = FPGrowth()\n",
    "fpm = fp.fit(df.select(df.basket.alias('items')))\n",
    "\n",
    "fpm.freqItemsets.sort(\"freq\", ascending=False).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e4611192-fbe8-458b-8e86-98e722a18988",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+------------------+------------------+-------+\n",
      "|antecedent|consequent|        confidence|              lift|support|\n",
      "+----------+----------+------------------+------------------+-------+\n",
      "|       [a]|     [cat]|               1.0|1.3333333333333333|  0.375|\n",
      "|       [a]|     [dog]|               1.0|1.1428571428571428|  0.375|\n",
      "|  [a, cat]|     [dog]|               1.0|1.1428571428571428|  0.375|\n",
      "|  [a, dog]|     [cat]|               1.0|1.3333333333333333|  0.375|\n",
      "|     [and]|     [cat]|               0.8|1.0666666666666667|    0.5|\n",
      "|     [and]|     [dog]|               0.8|0.9142857142857144|    0.5|\n",
      "|     [cat]|     [dog]|0.8333333333333334|0.9523809523809524|  0.625|\n",
      "+----------+----------+------------------+------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fpm.associationRules.sort(\"antecedent\", \"consequent\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5731c5e-acd3-4c71-9217-b92313f91668",
   "metadata": {},
   "source": [
    "It is built to scale for large datasets -- e.g. [Instacart's data of over 3 million orders on the platform](https://tech.instacart.com/3-million-instacart-orders-open-sourced-d40d29ead6f2) -- which can allow us to discover patterns and association rules in food buying that may point to deeper cultural forces at play."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eeec3261-f51f-4453-a375-26a67d986e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "instacart_orders = spark.read.csv('/project/macs40123/order_products_prior.csv', header=True)\n",
    "instacart_baskets = instacart_orders.groupBy('order_id').agg(F.collect_list('product_id').alias('basket'))\n",
    "# for demo with 8 cores, 5 GB RAM each, sample down:\n",
    "instacart_baskets = instacart_baskets.sample(0.01, seed=40123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b192b109-de04-4908-823d-bffe47df1b8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 23:===================================================>      (8 + 1) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+\n",
      "|  items|freq|\n",
      "+-------+----+\n",
      "|[24852]|4723|\n",
      "|[13176]|3793|\n",
      "|[21137]|2593|\n",
      "|[21903]|2418|\n",
      "|[47209]|2135|\n",
      "+-------+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "fp = FPGrowth(minConfidence=0.5, minSupport=0.001) # support = 3.21m * 0.001 = 321\n",
    "fpm = fp.fit(instacart_baskets.select(instacart_baskets.basket.alias('items')))\n",
    "fpm.freqItemsets.sort(\"freq\", ascending=False).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a15e8874-a9e5-41ff-bb3d-3fefadd32cd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 27:=============================================>            (7 + 2) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+----------+------------------+------------------+---------------------+\n",
      "|antecedent           |consequent|confidence        |lift              |support              |\n",
      "+---------------------+----------+------------------+------------------+---------------------+\n",
      "|[21709, 35221]       |[44632]   |0.524390243902439 |24.01497860199715 |0.0013394386817431392|\n",
      "|[21709, 44632]       |[35221]   |0.5119047619047619|37.519814090019565|0.0013394386817431392|\n",
      "|[25890, 49683]       |[24852]   |0.532258064516129 |3.6178447269026655|0.0010279413138958977|\n",
      "|[27966, 47209, 21137]|[13176]   |0.5176470588235295|4.381234782338984 |0.0013705884185278635|\n",
      "|[28204, 16797]       |[24852]   |0.5735294117647058|3.898372793961963 |0.0012148397346042426|\n",
      "|[33787, 18523]       |[33754]   |0.6923076923076923|73.83772041911577 |0.00112139052425007  |\n",
      "|[38544]              |[4962]    |0.5138888888888888|222.93749999999997|0.0011525402610347943|\n",
      "|[41065]              |[45007]   |0.5284552845528455|15.929577464788732|0.002024732891007071 |\n",
      "|[45066, 47626]       |[24852]   |0.5               |3.398581410120686 |0.0015263371024514844|\n",
      "|[4957, 33787]        |[33754]   |0.5441176470588235|58.03258745358608 |0.0011525402610347943|\n",
      "|[4962]               |[38544]   |0.5               |222.9375          |0.0011525402610347943|\n",
      "+---------------------+----------+------------------+------------------+---------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# lift > 1 (pos corr), lift < 1 (neg corr), lift == 1 (indep)\n",
    "fpm.associationRules.sort(\"antecedent\", \"consequent\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f8bd78cd-a28d-48dd-80f5-0faee63dd76a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------------------------------+\n",
      "|product_id|product_name                     |\n",
      "+----------+---------------------------------+\n",
      "|24852     |Banana                           |\n",
      "|25890     |Boneless Skinless Chicken Breasts|\n",
      "|49683     |Cucumber Kirby                   |\n",
      "+----------+---------------------------------+\n",
      "\n",
      "+----------+----------------------+\n",
      "|product_id|product_name          |\n",
      "+----------+----------------------+\n",
      "|13176     |Bag of Organic Bananas|\n",
      "|21137     |Organic Strawberries  |\n",
      "|27966     |Organic Raspberries   |\n",
      "|47209     |Organic Hass Avocado  |\n",
      "+----------+----------------------+\n",
      "\n",
      "+----------+-----------------------------------------------------+\n",
      "|product_id|product_name                                         |\n",
      "+----------+-----------------------------------------------------+\n",
      "|4962      |Yotoddler Organic Pear Spinach Mango Yogurt          |\n",
      "|38544     |Organic Whole Milk Strawberry Beet Berry Yogurt Pouch|\n",
      "+----------+-----------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "product_codes = spark.read.csv('/project/macs40123/products.csv', header=True)\n",
    "\n",
    "# discovery of a recipe? -- a particular type of cultural assemblage\n",
    "product_codes.select('product_id', 'product_name') \\\n",
    "             .where(F.col('product_id').isin([25890, 49683, 24852])) \\\n",
    "             .show(truncate=False)\n",
    "\n",
    "# grammar of *organic* fruit/vegetables: Avacado + Raspberries + Strawberries -> Bananas\n",
    "product_codes.select('product_id', 'product_name') \\\n",
    "             .where(F.col('product_id').isin([27966, 47209, 21137, 13176\n",
    "                                              ])) \\\n",
    "             .show(truncate=False)\n",
    "\n",
    "# indication of toddler food? \"Yotoddler\"\n",
    "product_codes.select('product_id', 'product_name') \\\n",
    "             .where(F.col('product_id').isin([4962, 38544\n",
    "                                              ])) \\\n",
    "             .show(truncate=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
