{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qwtlO4_m_LbQ"
   },
   "source": [
    "## Scalable Dimension Reduction and Clustering with Spark\n",
    "\n",
    "Today, we will be working [Spotify song data](https://www.google.com/url?q=https%3A%2F%2Fwww.kaggle.com%2Fdatasets%2Fjoebeachcapital%2F30000-spotify-songs%2Fdata) collected from Spotify API and publicly available on Kaggle. Our goal will be to construct coherent clusters that describe music based on perceived musical features -- allowing us to map the space of musical signs (beyond simply relying on their reported \"genre,\" which often fails to recognize cross-genre work).\n",
    "\n",
    "Spark has implementations of PCA and SVD, along with K-Means, so we will employ these methods in this notebook. For further detail on the methods, consult the MMDS textbook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 959,
     "status": "ok",
     "timestamp": 1729109195701,
     "user": {
      "displayName": "Jon Clindaniel",
      "userId": "17358510137887145828"
     },
     "user_tz": 300
    },
    "id": "twk-K-jilWK7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/10/16 20:30:44 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['track_id',\n",
       " 'track_name',\n",
       " 'track_artist',\n",
       " 'track_popularity',\n",
       " 'track_album_id',\n",
       " 'track_album_name',\n",
       " 'track_album_release_date',\n",
       " 'playlist_name',\n",
       " 'playlist_id',\n",
       " 'playlist_genre',\n",
       " 'playlist_subgenre',\n",
       " 'danceability',\n",
       " 'energy',\n",
       " 'key',\n",
       " 'loudness',\n",
       " 'mode',\n",
       " 'speechiness',\n",
       " 'acousticness',\n",
       " 'instrumentalness',\n",
       " 'liveness',\n",
       " 'valence',\n",
       " 'tempo',\n",
       " 'duration_ms']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession, Row\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import StandardScaler, PCA\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.evaluation import ClusteringEvaluator\n",
    "from pyspark.mllib.feature import StandardScaler as StandardScalerRDD\n",
    "from pyspark.mllib.linalg.distributed import RowMatrix\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .appName(\"dr_cluster\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "# Read Spotify data\n",
    "df = spark.read.csv('/project/macs40123/spotify_songs.csv', header=True)\n",
    "\n",
    "# Note potentially relevant features like danceability, energy, acousticness, etc.\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kAYRX2PMm0L6"
   },
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9487,
     "status": "ok",
     "timestamp": 1729118919934,
     "user": {
      "displayName": "Jon Clindaniel",
      "userId": "17358510137887145828"
     },
     "user_tz": 300
    },
    "id": "Oitav_xhQD9w",
    "outputId": "ff8992bd-2fed-42a7-e734-4d9a58e4f943"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 2:>                                                          (0 + 2) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- features: vector (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# identify potentially relevant features and add to a feature dataframe\n",
    "feature_cols = ['track_popularity', 'danceability', 'energy',\n",
    "                'key', 'loudness', 'speechiness',\n",
    "                'acousticness', 'instrumentalness', 'liveness',\n",
    "                'valence', 'tempo', 'duration_ms']\n",
    "\n",
    "# select feature columns and numeric data as floats\n",
    "df_features = df.select(*(F.col(c).cast(\"float\").alias(c) for c in feature_cols),'track_id', 'track_artist') \\\n",
    "                         .dropna()\n",
    "df_features = df_features.withColumn('features', F.array(*[F.col(c) for c in feature_cols])) \\\n",
    "                         .select('track_id', 'track_artist', 'features')\n",
    "\n",
    "# convert features to dense vector format (expected by K-Means, PCA)\n",
    "vectors = df_features.rdd.map(lambda row: Vectors.dense(row.features))\n",
    "features = spark.createDataFrame(vectors.map(Row), [\"features_unscaled\"])\n",
    "\n",
    "# scale features (some values like duration_ms are much larger than others)\n",
    "standardizer = StandardScaler(inputCol=\"features_unscaled\", outputCol=\"features\")\n",
    "model = standardizer.fit(features)\n",
    "features = model.transform(features) \\\n",
    "                .select('features')\n",
    "\n",
    "# persist in memory before fit model\n",
    "features.persist()\n",
    "features.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CRaF2A_j_nC7"
   },
   "source": [
    "### K-means\n",
    "\n",
    "Now, we could use the K-means clustering algorithm based on the features in our dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3534,
     "status": "ok",
     "timestamp": 1729112293333,
     "user": {
      "displayName": "Jon Clindaniel",
      "userId": "17358510137887145828"
     },
     "user_tz": 300
    },
    "id": "0xVIfPHZwWaE",
    "outputId": "553e8fb5-4d09-47f6-9866-668dbb7ef6c2"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Silhouette with squared euclidean distance = 0.18148415767740186\n"
     ]
    }
   ],
   "source": [
    "# train model\n",
    "kmeans = KMeans(k=3, seed=1)\n",
    "model = kmeans.fit(features)\n",
    "\n",
    "# make predictions (i.e. identify clusters)\n",
    "predictions = model.transform(features)\n",
    "\n",
    "# evaluate clustering by computing silhouette coef\n",
    "evaluator = ClusteringEvaluator()\n",
    "silhouette = evaluator.evaluate(predictions)\n",
    "print(\"Silhouette with squared euclidean distance = \" + str(silhouette))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fLIprM1JsdTU"
   },
   "source": [
    "This is not that great without first performing dimension reduction, though...\n",
    "\n",
    "### PCA\n",
    "\n",
    "Let's try to perform dimensionality reduction on the ```features``` -- using [PCA](https://spark.apache.org/docs/latest/ml-features.html#pca) before fitting our K-Means model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 559,
     "status": "ok",
     "timestamp": 1729112755636,
     "user": {
      "displayName": "Jon Clindaniel",
      "userId": "17358510137887145828"
     },
     "user_tz": 300
    },
    "id": "p4J8JMDkSb24",
    "outputId": "48430c17-0aea-4b35-e348-31ffec21fadf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/10/16 20:34:33 WARN LAPACK: Failed to load implementation from: com.github.fommil.netlib.NativeSystemLAPACK\n",
      "24/10/16 20:34:33 WARN LAPACK: Failed to load implementation from: com.github.fommil.netlib.NativeRefLAPACK\n",
      "+---------------------------------------+\n",
      "|features                               |\n",
      "+---------------------------------------+\n",
      "|[3.088270786158075,2.7825003774651256] |\n",
      "|[2.631248482088186,3.0145971151939386] |\n",
      "|[3.153704155957454,2.757676584083471]  |\n",
      "|[3.162586940929096,2.1786940454652988] |\n",
      "|[2.6209791405449754,2.6867473908798885]|\n",
      "+---------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# fit model\n",
    "pca = PCA(k=2, inputCol=\"features\", outputCol=\"pcaFeatures\")\n",
    "model = pca.fit(features)\n",
    "\n",
    "# transform feature data\n",
    "pca_results = model.transform(features).select(\"pcaFeatures\")\n",
    "pca_features = pca_results.rdd.map(lambda row: Vectors.dense(row.pcaFeatures))\n",
    "pca_features = spark.createDataFrame(pca_features.map(Row), [\"features\"])\n",
    "\n",
    "# persist data before training model on PCA-discovered features\n",
    "pca_features.persist()\n",
    "\n",
    "# Note: we've reduced our dimensionality down to 2 dimensions\n",
    "pca_features.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8leQR4-atMAl"
   },
   "source": [
    "Now let's run K-means with the same parameters as above, but on the ```pcaFeatures``` produced by the PCA reduction we just executed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4592,
     "status": "ok",
     "timestamp": 1729112811048,
     "user": {
      "displayName": "Jon Clindaniel",
      "userId": "17358510137887145828"
     },
     "user_tz": 300
    },
    "id": "U_snSSj5k2y5",
    "outputId": "6a76f7d1-0cab-4deb-f21c-2b89d2fa2144"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Silhouette with squared euclidean distance = 0.5410930506879614\n"
     ]
    }
   ],
   "source": [
    "# fit model\n",
    "pca_kmeans = KMeans(k=3, seed=1)\n",
    "pca_model = pca_kmeans.fit(pca_features)\n",
    "\n",
    "# make predictions (i.e. identify clusters)\n",
    "pca_predictions = pca_model.transform(pca_features)\n",
    "\n",
    "# evaluate clustering by computing silhouette coef\n",
    "pca_evaluator = ClusteringEvaluator()\n",
    "silhouette = pca_evaluator.evaluate(pca_predictions)\n",
    "print(\"Silhouette with squared euclidean distance = \" + str(silhouette))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vWkvIy7Pou5S"
   },
   "source": [
    "A bit better, but we can likely improve further. \n",
    "\n",
    "### SVD\n",
    "\n",
    "Recall from MMDS that Singular Value Decomposition (SVD) can be another powerful approach for dimension reduction. Let's use [Spark's SVD implementation](https://spark.apache.org/docs/latest/mllib-dimensionality-reduction#svd-example) here (which is implemented only for RDDs, so we will need to convert our DataFrame into an RDD to use it):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 27394,
     "status": "ok",
     "timestamp": 1729115367621,
     "user": {
      "displayName": "Jon Clindaniel",
      "userId": "17358510137887145828"
     },
     "user_tz": 300
    },
    "id": "uB7LblI_uHdm",
    "outputId": "331d6d1e-9709-4925-d0c0-cc9aff52161c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/10/16 20:35:35 WARN InstanceBuilder$NativeARPACK: Failed to load implementation from:dev.ludovic.netlib.arpack.JNIARPACK\n",
      "24/10/16 20:35:35 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS\n",
      "24/10/16 20:35:35 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeRefBLAS\n",
      "+---------------------------------------------+\n",
      "|features                                     |\n",
      "+---------------------------------------------+\n",
      "|[-0.005665575844501509,0.0052721985600063075]|\n",
      "|[-0.005602501869667561,0.0034811359031196414]|\n",
      "|[-0.005502030149423862,0.005737412004379039] |\n",
      "|[-0.005516497700975487,0.0058838999703511525]|\n",
      "|[-0.005453144073416807,0.003700134920677747] |\n",
      "+---------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# convert to RDD\n",
    "vectors_rdd = df_features.rdd.map(lambda row: row[\"features\"])\n",
    "\n",
    "# use RDD-specific standardizer to re-scale data\n",
    "standardizer_rdd = StandardScalerRDD()\n",
    "model = standardizer_rdd.fit(vectors_rdd)\n",
    "vectors_rdd = model.transform(vectors_rdd)\n",
    "mat = RowMatrix(vectors_rdd)\n",
    "\n",
    "# Compute SVD, retain 2 SVs to match 2 PCs of PCA\n",
    "svd = mat.computeSVD(2, computeU=True)\n",
    "\n",
    "# Access SVD components\n",
    "U = svd.U\n",
    "s = svd.s\n",
    "V = svd.V\n",
    "\n",
    "# convert U to DataFrame (and persist to memory) for clustering with K-Means\n",
    "U_df = U.rows.map(lambda row: Row(features=Vectors.dense(row.toArray()))) \\\n",
    "             .toDF()\n",
    "U_df.persist()\n",
    "\n",
    "# Note: we've reduced our dimensionality down to 2 dimensions again\n",
    "U_df.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run K-means once more with the same parameters as above, but on ```U_df``` produced by the SVD reduction we just executed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3361,
     "status": "ok",
     "timestamp": 1729115463152,
     "user": {
      "displayName": "Jon Clindaniel",
      "userId": "17358510137887145828"
     },
     "user_tz": 300
    },
    "id": "h41Jo0a7Qw7D",
    "outputId": "58bc11de-3311-4593-dae9-f6ed06309a7a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Silhouette with squared euclidean distance = 0.6978275869985402\n"
     ]
    }
   ],
   "source": [
    "# train model\n",
    "svd_kmeans = KMeans(k=3, seed=1)\n",
    "svd_model = svd_kmeans.fit(U_df)\n",
    "\n",
    "# make predictions (i.e. identify clusters)\n",
    "svd_predictions = svd_model.transform(U_df)\n",
    "\n",
    "# evaluate clustering by computing silhouette score\n",
    "svd_evaluator = ClusteringEvaluator()\n",
    "silhouette = svd_evaluator.evaluate(svd_predictions)\n",
    "print(\"Silhouette with squared euclidean distance = \" + str(silhouette))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is quite a bit better than our previous two tries. Let's take a closer look at the resulting clusters. Recall that we often don't want to plot all of our data points when we're working at scale (this can result in overplotting and we want to perform as many computations in parallel on our cluster before bringing data back to our primary node and risking that we run out of memory). Here, we are working with a smaller dataset, but we will apply the same logic of working at scale. \n",
    "\n",
    "For instance, we can take a look at how many items are in each cluster:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 303,
     "status": "ok",
     "timestamp": 1729115674850,
     "user": {
      "displayName": "Jon Clindaniel",
      "userId": "17358510137887145828"
     },
     "user_tz": 300
    },
    "id": "DrYePqPzUrS9",
    "outputId": "9a932af0-27fb-498a-ced6-3e3f9cf96157"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n",
      "|prediction|count|\n",
      "+----------+-----+\n",
      "|         1| 4336|\n",
      "|         2|13584|\n",
      "|         0|14901|\n",
      "+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "svd_predictions.groupby('prediction') \\\n",
    "               .count() \\\n",
    "               .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also merge our cluster information back with the song IDs and track artists (and any other data about the songs that you would like to investigate):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 724,
     "status": "ok",
     "timestamp": 1729118944580,
     "user": {
      "displayName": "Jon Clindaniel",
      "userId": "17358510137887145828"
     },
     "user_tz": 300
    },
    "id": "kDX-jd8Nb2Gr",
    "outputId": "760dfec4-d537-4f60-b393-2f917db56f88"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+----------------+--------------------+--------------------+----------+\n",
      "| id|            track_id|    track_artist|            features|            features|prediction|\n",
      "+---+--------------------+----------------+--------------------+--------------------+----------+\n",
      "|  0|6f807x0ima9a1j3VP...|      Ed Sheeran|[66.0, 0.748, 0.9...|[-0.0056655758445...|         0|\n",
      "|  1|0r7CVbZTWZgbTCYdf...|        Maroon 5|[67.0, 0.726, 0.8...|[-0.0056025018696...|         0|\n",
      "|  2|1z1Hg7Vb0AhHDiEmn...|    Zara Larsson|[70.0, 0.675, 0.9...|[-0.0055020301494...|         0|\n",
      "|  3|75FpbthrwQmzHlBJL...|The Chainsmokers|[60.0, 0.718, 0.9...|[-0.0055164977009...|         0|\n",
      "|  4|1e8PAfcKUYoKkxPhr...|   Lewis Capaldi|[69.0, 0.65, 0.83...|[-0.0054531440734...|         0|\n",
      "+---+--------------------+----------------+--------------------+--------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Add an index to U_df that matches df_features (to enable merging)\n",
    "df_features_with_id = df_features.withColumn(\"id\", F.monotonically_increasing_id())\n",
    "svd_predictions_with_id = svd_predictions.withColumn(\"id\", F.monotonically_increasing_id())\n",
    "\n",
    "# Perform an inner join on the 'id' column to merge df_features with U_df\n",
    "df_merged = df_features_with_id.join(svd_predictions_with_id, on=\"id\", how=\"inner\")\n",
    "df_merged.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, considering only artists of the songs, it is clear that there are discernable patterns in the way in which the clusters have been defined -- seemingly, clustering has identified a DJ-specific cluster, a singer/song-writer cluster, as well as a cluster that leans more toward rap, reggaeton (and perhaps DJs who sample this music):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9837,
     "status": "ok",
     "timestamp": 1729119829526,
     "user": {
      "displayName": "Jon Clindaniel",
      "userId": "17358510137887145828"
     },
     "user_tz": 300
    },
    "id": "1jYHK13Dhd_s",
    "outputId": "7e1a96c2-146e-4aa5-f6c0-ecd5280b2614"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+-----+\n",
      "|prediction|        track_artist|count|\n",
      "+----------+--------------------+-----+\n",
      "|         0|       Martin Garrix|  120|\n",
      "|         0|Dimitri Vegas & L...|   91|\n",
      "|         0|        David Guetta|   86|\n",
      "|         0|            Hardwell|   76|\n",
      "|         0|       Calvin Harris|   76|\n",
      "+----------+--------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n",
      "+----------+-------------+-----+\n",
      "|prediction| track_artist|count|\n",
      "+----------+-------------+-----+\n",
      "|         1|Billie Eilish|   40|\n",
      "|         1|        Queen|   33|\n",
      "|         1|  Frank Ocean|   25|\n",
      "|         1|       Khalid|   23|\n",
      "|         1|Daniel Caesar|   23|\n",
      "+----------+-------------+-----+\n",
      "only showing top 5 rows\n",
      "\n",
      "+----------+----------------+-----+\n",
      "|prediction|    track_artist|count|\n",
      "+----------+----------------+-----+\n",
      "|         2|           Drake|   69|\n",
      "|         2|The Chainsmokers|   62|\n",
      "|         2|            Kygo|   62|\n",
      "|         2|           Queen|   60|\n",
      "|         2|       Bad Bunny|   45|\n",
      "|         2|      The Weeknd|   44|\n",
      "|         2|   Martin Garrix|   41|\n",
      "|         2|      Ed Sheeran|   41|\n",
      "|         2|      Young Thug|   38|\n",
      "|         2|          Future|   37|\n",
      "+----------+----------------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cluster_artist_count = df_merged.groupby(['prediction', 'track_artist']) \\\n",
    "                                .count() \\\n",
    "                                .orderBy(['count', 'track_artist'],\n",
    "                                         ascending=False)\n",
    "# DJs\n",
    "cluster_artist_count.filter(F.col('prediction') == 0) \\\n",
    "                    .show(5)\n",
    "\n",
    "# Singer/Song-writers\n",
    "cluster_artist_count.filter(F.col('prediction') == 1) \\\n",
    "                    .show(5)\n",
    "\n",
    "# Some rap, reggaeton, (DJs sampling?)\n",
    "cluster_artist_count.filter(F.col('prediction') == 2) \\\n",
    "                    .show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's all for now, but you're encouraged to dig further into the specific songs within each cluster to describe the logic of the clusters in more detail on your own!"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [
    {
     "file_id": "1F9_DbM919MlpY7WnlkOHdtXniQrWte9Q",
     "timestamp": 1728947827824
    },
    {
     "file_id": "1Tu7xeYM0qBB-RkqLSnYGrIyzb3q7bYo1",
     "timestamp": 1681987493112
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
